{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate metrics Manually\n",
    "* we will work on Titanic survival database\n",
    "* a simple KNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# load data\n",
    "titanic = sns.load_dataset('titanic')\n",
    "# transform 'sex' column to numerical values\n",
    "titanic.iloc[:, 2] = LabelEncoder().fit_transform(titanic.iloc[:, 2].values)\n",
    "# Drop null values\n",
    "titanic = titanic.dropna()\n",
    "# Create X and y\n",
    "X = titanic.iloc[:, 1:4].values\n",
    "Y = titanic.iloc[:, 0].values\n",
    "# split data into train (80%), and test (20%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "# create an object from KNeiborsClassifier class and fit the model woth training data\n",
    "knn = KNeighborsClassifier().fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# make prediction save them in y_pred\n",
    "y_pred = knn.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* True positive (TP) = the number of cases correctly identified as positive\n",
    "* False positive (FP) = the number of cases incorrectly identified as positive\n",
    "* True negative (TN) = the number of cases correctly identified as negative\n",
    "* False negative (FN) = the number of cases incorrectly identified as negative"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN: Nombre de True negatives =  4\n",
      "FP: Nombre de False positives =  6\n",
      "FN: Nombre de False negatives =  7\n",
      "TP: Nombre de True positives =  20\n"
     ]
    }
   ],
   "source": [
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_pred[i] == 1 and Y_test[i]==1:\n",
    "        TP+=1\n",
    "    if y_pred[i] == 0 and Y_test[i]==0:\n",
    "        TN+=1\n",
    "    if y_pred[i] == 1 and Y_test[i]==0:\n",
    "        FP+=1\n",
    "    if y_pred[i] == 0 and Y_test[i]==1:\n",
    "        FN+=1\n",
    "\n",
    "print('TN: Nombre de True negatives = ' , TN)\n",
    "print('FP: Nombre de False positives = ' , FP)\n",
    "print('FN: Nombre de False negatives = ' , FN)\n",
    "print('TP: Nombre de True positives = ' , TP)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy\n",
    "The accuracy of a test is its ability to differentiate the patient and healthy cases correctly. To estimate the accuracy of a test, we should calculate the proportion of true positive and true negative in all evaluated cases.\n",
    "\n",
    "*Mathematically, this can be stated as:* **Accuracy=TP+TN/(TP+TN+FP+FN)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuraccy = 0.6486\n"
     ]
    }
   ],
   "source": [
    "accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "print('accuracy = %.4f' %accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy metric = 64.8649\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy percentage\n",
    "cnt = 0\n",
    "for i in range(len(Y_test)):\n",
    "    if Y_test[i] == y_pred[i]:\n",
    "        cnt += 1\n",
    "accuracy_metric = cnt / float(len(Y_test)) * 100.0\n",
    "print('Accuracy metric = %.4f' %accuracy_metric)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sensitivity or Recall\n",
    "The sensitivity of a test is its ability to determine the patient cases correctly. To estimate it, we should calculate the proportion of true positive in patient cases.\n",
    "*Mathematically, this can be stated as:* **Sensitivity=TP/(TP+FN)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitivity = 0.7407\n"
     ]
    }
   ],
   "source": [
    "sensitivity = TP/(TP+FN)\n",
    "print('Sensitivity = %.4f' %sensitivity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Specificity\n",
    "The specificity of a test is its ability to determine the healthy cases correctly. To estimate it, we should calculate the proportion of true negative in healthy cases.\n",
    "*Mathematically, this can be stated as:* **Specificity=TN/(TN+FP)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity = 0.4000\n"
     ]
    }
   ],
   "source": [
    "specificity = TN/(TN+FP)\n",
    "print('Specificity = %.4f' %specificity)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Precision\n",
    "Mathematically : **Precision = TP  / FP + TP**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7692\n"
     ]
    }
   ],
   "source": [
    "precision = TP/(TP + FP)\n",
    "print('Precision = %.4f' %precision)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RMSE\n",
    "RMSE is calculated as the square root of the mean of the squared differences between actual outcomes and predictions.\n",
    "Mathematically: **RMSE = sqrt( sum( (predicted_i - real_i)^2 ) / total predictions)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.5927\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "sum_error = 0.0\n",
    "for i in range(len(Y_test)):\n",
    "    prediction_error = y_pred[i] - Y_test[i]\n",
    "    sum_error += (prediction_error ** 2)\n",
    "mean_error = sum_error / float(len(Y_test))\n",
    "rmse = sqrt(mean_error)\n",
    "print('RMSE = %.4f' %rmse)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MAE\n",
    "The Mean Absolute Error or MAE for short is a good first error metric to use. It is calculated as the average of the absolute error values, where “absolute” means “made positive” so that they can be added together.\n",
    "Mathematically:  **MAE = sum( abs(predicted_i - real_i) ) / total predictions**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 0.3514\n"
     ]
    }
   ],
   "source": [
    "sum_error = 0.0\n",
    "for i in range(len(Y_test)):\n",
    "    sum_error += abs(y_pred[i] - Y_test[i])\n",
    "mae = sum_error / float(len(Y_test))\n",
    "print('MAE = %.4f' %mae)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}